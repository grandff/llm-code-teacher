{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from celery import Celery\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from ollama import Client\n",
    "import httpx\n",
    "from langchain_community.chat_models.ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# markdown 파일 생성\n",
    "def save_md_to_file(markdown_str : str, directory : str, file_name : str) :\n",
    "    # 디렉토리가 존재하지 않으면 생성\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        \n",
    "     # 파일 경로 생성\n",
    "    file_path = os.path.join(directory, file_name)\n",
    "    \n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(markdown_str)\n",
    "    \n",
    "    # 파일 객체 정보 변환\n",
    "    file_info = {\n",
    "        'file_path' : file_path,\n",
    "        'file_name' : os.path.getsize(file_path)\n",
    "    }\n",
    "    \n",
    "    return file_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 파일당 하나씩 프롬프트 생성하기\n",
    "user_prompt = '''\n",
    "You are an experienced technical writer skilled in documenting and formatting code reviews. \n",
    "Your task is to take the results from a code review and format them into a markdown file. \n",
    "The markdown file must be written in English and should include the following sections: Analysis Summary, Key Features, Pre-condition Check, Runtime Error Check, Optimization, Security Issue, and Evaluation. \n",
    "Each section should be formatted appropriately:\n",
    "\n",
    "- **Analysis Summary:** Summarize the code review findings in one or two lines.\n",
    "- **Key Features:** Analyze and describe the key features of the added or modified files.\n",
    "- **Precondition checks:** Checks that a function or method has the necessary variable states or ranges of values to function correctly.\n",
    "- **Runtime error checking:** Examines code for possible runtime errors and identifies other potential risks.\n",
    "- **Optimization:** Scan your code and recommend optimized code. When recommending code, be sure to include the full source of the file. Please write your code using code blocks to conform to the markdown format - this is a must. \n",
    "- **Security issues:** Scans your code to see if it uses modules with serious security flaws or contains security vulnerabilities.\n",
    "- **Evaluation:** Comprehensively evaluates your work. Consider the quality, functionality, and maintainability of the code.\n",
    "            \n",
    "Ensure the markdown document is clear, well-structured, and easy to read.\n",
    "```python\n",
    "# main.py\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "from fastapi.requests import Request\n",
    "from fastapi import FastAPI, HTTPException, Query, Header, Depends\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain.schema import StrOutputParser\n",
    "import time\n",
    "from urllib.parse import urlparse\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import SystemMessage\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    model=\"gpt-4o\",  \n",
    "    openai_api_key=\"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\"\n",
    ")\n",
    "\n",
    "# .env 파일에서 환경 변수 로드\n",
    "load_dotenv()\n",
    "\n",
    "# .env 파일에서 EXPECTED_VALUE 변수 가져오기\n",
    "EXPECTED_VALUE = os.getenv(\"EXPECTED_VALUE\")\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Dictionary to keep track of successfully parsed URLs\n",
    "parsed_urls = {}\n",
    "\n",
    "\n",
    "def extract_data_from_script(soup, script_name):\n",
    "    # Find the script tag containing the desired variable\n",
    "    script_tag = soup.find('script', string=re.compile(f\"{script_name}\"))\n",
    "\n",
    "    if script_tag:\n",
    "        # Extract the variable's value using regex\n",
    "        match = re.search(f\"{script_name} = (.*);\", script_tag.string)\n",
    "        if match:\n",
    "            data_json = match.group(1)\n",
    "            return data_json\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_specific_values(data):\n",
    "    specific_values = {}\n",
    "\n",
    "    def find_values(obj):\n",
    "        if isinstance(obj, dict):\n",
    "            for key, value in obj.items():\n",
    "                if \"Menu\" in key or \"PlaceDetailBase\" in key:\n",
    "                    specific_values[key] = value\n",
    "                find_values(value)\n",
    "        elif isinstance(obj, list):\n",
    "            for item in obj:\n",
    "                find_values(item)\n",
    "\n",
    "    find_values(data)\n",
    "    return specific_values\n",
    "\n",
    "\n",
    "def extract_specific_values_v2(data):\n",
    "    specific_values = {}\n",
    "\n",
    "    def find_values(obj):\n",
    "        if isinstance(obj, dict):\n",
    "            for key, value in obj.items():\n",
    "                if \"TripSummary\" in key:\n",
    "                    specific_values[key] = value\n",
    "                find_values(value)\n",
    "        elif isinstance(obj, list):\n",
    "            for item in obj:\n",
    "                find_values(item)\n",
    "\n",
    "    find_values(data)\n",
    "    return specific_values\n",
    "\n",
    "# Define a custom dependency to check the header value\n",
    "\n",
    "\n",
    "def check_header_value(x_custom_header: str = Header(None, convert_underscores=True)):\n",
    "    if x_custom_header != \"expected_value\":\n",
    "        raise HTTPException(status_code=400, detail=\"Invalid header value\")\n",
    "    return x_custom_header\n",
    "\n",
    "# Middleware to check the header value for all requests\n",
    "\n",
    "\n",
    "@app.middleware(\"http\")\n",
    "async def check_header_middleware(request: Request, call_next):\n",
    "    x_custom_header = request.headers.get(\"x-custom-header\")\n",
    "    if x_custom_header != \"expected_value\":\n",
    "        raise HTTPException(status_code=400, detail=\"Invalid header value\")\n",
    "    response = await call_next(request)\n",
    "    return response\n",
    "\n",
    "\n",
    "@app.middleware(\"https\")\n",
    "async def check_header_middleware(request: Request, call_next):\n",
    "    x_custom_header = request.headers.get(\"x-custom-header\")\n",
    "    if x_custom_header != \"expected_value\":\n",
    "        raise HTTPException(status_code=400, detail=\"Invalid header value\")\n",
    "    response = await call_next(request)\n",
    "    return response\n",
    "\n",
    "# 랜덤 프록시\n",
    "def random_us_proxy() :\n",
    "    proxy_url = \"https://www.us-proxy.org/\"\n",
    "\n",
    "    res = requests.get(proxy_url)\n",
    "    soup = BeautifulSoup(res.text,'lxml')\n",
    "\n",
    "    table = soup.find('tbody')\n",
    "    rows = table.find_all('tr')\n",
    "    proxy_server_list = []\n",
    "\n",
    "    for row in rows:\n",
    "        https = row.find('td', attrs = {'class':'hx'})\n",
    "        if https.text == 'yes':\n",
    "            ip = row.find_all('td')[0].text\n",
    "            port = row.find_all('td')[1].text\n",
    "            server = f\"{ip}:{port}\"\n",
    "            proxy_server_list.append(server)\n",
    "\n",
    "    proxy_server = random.choices(proxy_server_list)[0]\n",
    "    return proxy_server\n",
    "\n",
    "\n",
    "@app.get(\"/bot/place\")\n",
    "async def get_place_info(keyword: str = Query(..., title=\"keyword\"), x_custom_header: str = Depends(check_header_value)):\n",
    "    if not keyword:\n",
    "        raise HTTPException(status_code=403, detail=\"No keyword provided\")\n",
    "\n",
    "    # URL-encode the keyword\n",
    "    # encoded_keyword = keyword.encode('utf-8').decode('latin1')\n",
    "\n",
    "    # 딜레이\n",
    "    time.sleep(1.3)\n",
    "    url = f\"https://m.search.naver.com/search.naver?sm=mtp_sly.hst&where=m&query={keyword}&acr=1\"    \n",
    "    print(f\"url : {url}\")\n",
    "\n",
    "    # Skip parsing if URL has been parsed successfully or failed before\n",
    "    if url in parsed_urls:\n",
    "        if parsed_urls[url] == \"success\":\n",
    "            return parsed_urls[url]\n",
    "        else:            \n",
    "            raise HTTPException(\n",
    "                status_code=404, detail=\"Parsing previously failed\")\n",
    "\n",
    "    # 프록시 추가        \n",
    "    # proxy_server = random_us_proxy()\n",
    "    # proxies = {\"http\": 'http://' + proxy_server, 'https': 'http://' + proxy_server}\n",
    "\n",
    "    response = requests.get(url)\n",
    "    print(f\"response status_code  : {response.status_code}\")\n",
    "    if response.status_code != 200:\n",
    "        parsed_urls[url] = \"fail\"\n",
    "        print(response.content)\n",
    "        raise HTTPException(status_code=404, detail=\"Parsing failed\")\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    target_tags = soup.find_all('a', href=lambda href: href and href.startswith(\n",
    "        \"https://m.place.naver.com/place\"))\n",
    "    \n",
    "    results = []\n",
    "\n",
    "    # target_tags에 중복된 값 제거\n",
    "    target_tags = list(set(target_tags))\n",
    "\n",
    "    for tag in target_tags:\n",
    "        href = tag['href']  # Get the href attribute of the tag as a string\n",
    "        # Remove \"/photo?entry=pll\" from href if present\n",
    "        if \"/photo\" in href:\n",
    "            href = href.replace(\"/photo\", \"\")\n",
    "        if \"/home\" in href:\n",
    "            href = href.replace(\"/home\", \"\")\n",
    "\n",
    "        # Skip parsing if URL has been parsed successfully or failed before\n",
    "        # if href in parsed_urls:\n",
    "        #    print(\"already parsed\")\n",
    "        #    continue\n",
    "            \n",
    "\n",
    "        # 딜레이\n",
    "        time.sleep(1.5)\n",
    "\n",
    "        payload = { 'api_key': 'bfc02298429da30d46250183fb72683b', 'url': href }\n",
    "        response = requests.get('https://api.scraperapi.com/', params=payload)    \n",
    "\n",
    "        # proxy_server = random_us_proxy()\n",
    "        # proxies = {\"http\": 'http://' + proxy_server, 'https': 'http://' + proxy_server}\n",
    "        # response = requests.get(href, proxies=proxies)\n",
    "        print(f\"href : {href}\")\n",
    "        # print(f\"response : {response.content}\")\n",
    "        print(f\"response status_code  : {response.status_code}\")\n",
    "  \n",
    "        if response.status_code != 200:\n",
    "            parsed_urls[href] = \"fail\"\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Extract the JSON data from window.__APOLLO_STATE__\n",
    "        data_json = extract_data_from_script(soup, \"window.__APOLLO_STATE__\")\n",
    "        # print(f\"data_json : {data_json}\")        \n",
    "\n",
    "        if data_json:\n",
    "            # Parse the JSON data\n",
    "            data = json.loads(data_json)\n",
    "\n",
    "            # Extract specific values as key-value pairs\n",
    "            specific_values = extract_specific_values(data)\n",
    "            # print(f\"data : {data}\")\n",
    "            print(f\"specific_values : {specific_values}\")\n",
    "\n",
    "            results.append(specific_values)\n",
    "            parsed_urls[href] = \"success\"\n",
    "\n",
    "    if not results:\n",
    "        parsed_urls[url] = \"fail\"\n",
    "        print(\"fail to find\")\n",
    "        raise HTTPException(status_code=404, detail=\"No relevant data found\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "@app.get(\"/bot/trend\")\n",
    "async def get_place_info(keyword: str = Query(..., title=\"keyword\"), x_custom_header: str = Depends(check_header_value)):\n",
    "    if not keyword:\n",
    "        raise HTTPException(status_code=403, detail=\"No keyword provided\")\n",
    "\n",
    "    results = []\n",
    "    url = f\"https://trip.place.naver.com/list?query={keyword}&theme=6&x=126.9783882&y=37.5666103&level=top\"\n",
    "    response = requests.get(url)\n",
    "    # 크롤링 실패 시\n",
    "    if response.status_code != 200:\n",
    "        print(\"fail cant find\")\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Extract the JSON data from window.__APOLLO_STATE__\n",
    "    data_json = extract_data_from_script(soup, \"window.__APOLLO_STATE__\")\n",
    "\n",
    "    if data_json:\n",
    "        # Parse the JSON data\n",
    "        data = json.loads(data_json)\n",
    "\n",
    "        # Extract specific values as key-value pairs\n",
    "        specific_values = extract_specific_values_v2(data)\n",
    "\n",
    "        results.append(specific_values)\n",
    "\n",
    "    if not results:\n",
    "        parsed_urls[url] = \"fail\"\n",
    "        print(\"fail to find\")\n",
    "        raise HTTPException(status_code=404, detail=\"No relevant data found\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "@app.get(\"/bot/blog\")\n",
    "async def get_blog_info(keyword: str = Query(..., title=\"keyword\"), x_custom_header: str = Depends(check_header_value)):\n",
    "    if not keyword:\n",
    "        raise HTTPException(status_code=403, detail=\"No keyword provided\")\n",
    "    # naver crawling\n",
    "    result, url_result = crawling_naver_blog(keyword=keyword)\n",
    "\n",
    "    if result == []:\n",
    "        raise HTTPException(status_code=404, detail=\"No Crawling result\")\n",
    "\n",
    "    # llm 랭체인 생성\n",
    "    # create prompt\n",
    "    summary_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"\"\"\n",
    "                당신은 사용자들에게 여행지 정보를 요약해주는 도우미입니다.\n",
    "                아래는 {keyword}에 대한 최근 글을 모아놓은 데이터입니다.                                         \n",
    "                \n",
    "                제공된 데이터를 사용자가 이해하기 쉽도록 최소 3개에서 최대 10개까지로 요약해주세요.\n",
    "                중복된 내용은 모두 제거해주세요.\n",
    "                제공된 데이터에 없는 내용은 지어내지 말아주세요.   \n",
    "                {keyword}에 관한 내용만 요약해주세요.\n",
    "                \n",
    "                각 특징은 구분할 수 있게, 개행문자로 나눠주세요.                \n",
    "                문장의 끝은 해요체로 끝나야 합니다.\n",
    "                                \n",
    "                ------------\n",
    "                {context}\n",
    "                ------------\n",
    "                \"\"\",\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # langchain\n",
    "    # create chain\n",
    "    summary_chain = summary_prompt | llm | StrOutputParser()\n",
    "    summary_result = summary_chain.invoke({\n",
    "        \"keyword\": keyword,\n",
    "        \"context\": format_docs(result),\n",
    "    })\n",
    "    return {\"result\": summary_result, \"urls\": url_result}\n",
    "\n",
    "\n",
    "def crawling_naver_blog(keyword):\n",
    "    blog_result = []\n",
    "    url_result = []\n",
    "    # 1. 검색어로 블로그 목록 조회\n",
    "    url = f\"https://m.search.naver.com/search.naver?ssc=tab.m_blog.all&sm=mtb_jum&query={keyword}\"\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(\"fail cant find\")\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    target_tags = soup.find_all('ul', class_='lst_view')\n",
    "    if not target_tags:\n",
    "        print(\"fail cant find2\")\n",
    "        return []\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # 각 ul 태그에 대해\n",
    "    for ul in target_tags:\n",
    "        # li 태그를 모두 찾습니다.\n",
    "        li_tags = ul.find_all('li')\n",
    "        for li in li_tags:\n",
    "            # li > div.view_wrap > div.detail_box > div.title_area > a 태그를 찾습니다.\n",
    "            a_tag = li.select_one(\n",
    "                'div.view_wrap > div.detail_box > div.title_area > a')\n",
    "            if a_tag is not None:\n",
    "                # href와 inner HTML 데이터를 가져옵니다.\n",
    "                href = a_tag.get('href')\n",
    "                # 결과에 추가합니다.\n",
    "                results.append(href)\n",
    "\n",
    "    if results.__len__() == 0:\n",
    "        print(\"fail cant find3\")\n",
    "        return []\n",
    "\n",
    "    # 2, 3, 4는 실제 백엔드에서 구현해야함.\n",
    "    # 5. 해당 url로 scraper api 호출\n",
    "    # 최근꺼 2개만 조회하자    \n",
    "    for result in results[:3]:\n",
    "        parsed_url = urlparse(result)\n",
    "\n",
    "        body = \"\"\n",
    "        # 도메인 다음에 오는 데이터\n",
    "        path = parsed_url.path\n",
    "        # '/'를 기준으로 데이터를 분리해서 리스트에 담기\n",
    "        path_list = path.split('/')[1:]\n",
    "\n",
    "        # 각 데이터를 변수에 담기\n",
    "        try:\n",
    "            var1, var2 = path_list\n",
    "        except ValueError:\n",
    "            print(\"parse error..!\")\n",
    "            continue\n",
    "\n",
    "        new_naver_url = f\"https://m.blog.naver.com/PostView.naver?blogId={var1}&logNo={var2}&proxyReferer=https%3A%2F%2Fm.search.naver.com%2F\"\n",
    "\n",
    "        # # blog_result에 이미 수집한 url인 경우 넘어감\n",
    "        if new_naver_url in url_result:\n",
    "            print(\"already parsed\")\n",
    "            continue\n",
    "\n",
    "        r = requests.post(url='https://async.scraperapi.com/jobs',\n",
    "                          json={'apiKey': 'f8fc33de41c4641274044d4fe69380f4', 'urls': [new_naver_url]})\n",
    "\n",
    "        # 주어진 JSON 응답\n",
    "        response_json = json.loads(r.text)\n",
    "\n",
    "        # statusUrl 값 추출\n",
    "        status_url = response_json[0]['statusUrl']\n",
    "\n",
    "        # statusUrl로 HTTP 요청 보내기\n",
    "        # 완료가 될때 까지 반복\n",
    "        # 아직 진행 중이면 2초 후 다시 요청\n",
    "        # 최대 5번까지 실행\n",
    "        for i in range(5):\n",
    "            response = requests.get(status_url)\n",
    "            response_json = json.loads(response.text)\n",
    "            if response_json['status'] == 'finished':\n",
    "                body = response_json['response']['body']\n",
    "                break\n",
    "            else:\n",
    "                time.sleep(2)\n",
    "\n",
    "        if body != \"\":\n",
    "            # body를 BeautifulSoup으로 파싱\n",
    "            soup = BeautifulSoup(body, 'html.parser')\n",
    "            # div._postView > div.post_ct > div.se-viewer > div.se-main-container > div.se-text 만 전부 추출\n",
    "            target_tags = soup.select(\n",
    "                'div._postView > div.post_ct > div.se-viewer > div.se-main-container > div.se-text')\n",
    "            # html 태그와 주석 처리를 모두 제거하고 텍스트만 남긴다음, 텍스트를 하나로 합침\n",
    "            result = ' '.join([tag.text.strip() for tag in target_tags])\n",
    "            blog_result.append(result)\n",
    "            url_result.append(new_naver_url)\n",
    "        time.sleep(3)\n",
    "    # blog_result의 중복값 제거\n",
    "    blog_result = list(set(blog_result))\n",
    "    return blog_result, url_result\n",
    "\n",
    "\n",
    "def format_docs(result):\n",
    "    # blog_result를 개행문자를 넣어서 하나로 합침\n",
    "    # document formatting\n",
    "    return \"\\n\\n\".join(result)  # 개행라인 추가\n",
    "\n",
    "\n",
    "@app.get(\"/bot/blog/agent\")\n",
    "async def get_blog_info_agent(keyword: str = Query(..., title=\"keyword\"), x_custom_header: str = Depends(check_header_value)):\n",
    "    if not keyword:\n",
    "        raise HTTPException(status_code=403, detail=\"No keyword provided\")\n",
    "    # naver crawling\n",
    "    result, url_result = crawling_naver_blog(keyword=keyword)\n",
    "\n",
    "    if result == []:\n",
    "        raise HTTPException(status_code=404, detail=\"No Crawling result\")\n",
    "\n",
    "    agent = initialize_agent(\n",
    "        llm=llm,\n",
    "        handle_parsing_erros=True,\n",
    "        agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "        # 시스템 프롬프트 변경\n",
    "        agent_kwargs={\n",
    "            \"system_message\": SystemMessage(\n",
    "                content=\"\"\"\n",
    "                당신은 사용자들에게 여행지 정보를 요약해주는 도우미입니다.\n",
    "                아래는 {keyword}를(을) 다녀온 블로거들이 작성한 블로그 글을 모아놓은 데이터입니다.                                         \n",
    "                \n",
    "                제공된 데이터를 사용자가 이해하기 쉽도록 최소 3개에서 최대 10개까지로 요약해주세요.\n",
    "                중복된 내용은 모두 제거해주세요.\n",
    "                제공된 데이터에 없는 내용은 지어내지 말아주세요.   \n",
    "                볼로거의 가족들에 관한 이야기는 빼고, {keyword}에 관한 내용만 사용해주세요.\n",
    "                \n",
    "                각 특징은 구분할 수 있게, 개행문자로 나눠주세요.                \n",
    "                문장의 끝은 해요체로 끝나야 합니다.\n",
    "                                \n",
    "                ------------\n",
    "                {context}\n",
    "                ------------\n",
    "                \"\"\",\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    summary_result = agent.invoke({\n",
    "        \"keyword\": keyword,\n",
    "        \"context\": format_docs(result),\n",
    "    })\n",
    "    return {\"result\": summary_result[\"output\"], \"urls\": url_result}\n",
    "```\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm 생성\n",
    "llama_llm = ChatOllama(           \n",
    "    base_url = \"http://localhost:11434\",\n",
    "    model=\"llama3.1\",\n",
    "    temperature=0,\n",
    ")     \n",
    "mistral_llm = ChatOllama(\n",
    "    base_url = \"http://localhost:11434\",\n",
    "    model=\"mistral\",\n",
    "    temperature=0,\n",
    ")     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt 생성\n",
    "code_review_messages = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"\n",
    "    You're a very good software analyst. From now on, users will show you the entire committed source code. Take a look at it, analyze it, and tell us what you find. \n",
    "    After looking at the source code, present an optimized version of the code, including performance improvements. \n",
    "\n",
    "    As you analyze the source code, keep the following topics in mind as you do so\n",
    "    - Analysis summary: Summarize your code review findings in one or two lines.\n",
    "    - Key features: Analyze and describe the key features of each file.\n",
    "    - Prerequisite checks: Verify that the function or method has the necessary variable states or value ranges to function correctly.\n",
    "    - Runtime error checking: Inspect your code for possible runtime errors and identify other potential risks.\n",
    "    - Optimization: Scans code patches for optimization points and recommends optimized code if it appears to be degrading performance. \n",
    "    - Security issues: Scan your code to see if it uses modules with serious security flaws or contains security vulnerabilities.\n",
    "    - Evaluation: Evaluate your work comprehensively. Considers the quality, functionality, and maintainability of the code.\n",
    "\n",
    "    Return a response in markdown so that your analysis is easy to parse.\n",
    "    The topics above are the same as the subheadings in your final analysis. In particular, be sure to write the entire code in the form of code blocks in Markdown for optimizations.\n",
    "    The Markdown documentation must be written in Korean.\n",
    "    Do not write any additional text other than the response values in Markdown format.\n",
    "    \"\"\"),\n",
    "    (\"human\", \"{prompt}\")\n",
    "])\n",
    "translate_messages = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"\n",
    "    You are an excellent translator. Please translate the markdown written by the user into Korean. \n",
    "    At this time, for the content in the code block, only the annotation should be translated, and the code should be left as it is.\n",
    "    \"\"\"),\n",
    "    (\"human\", \"{review_result}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain 생성\n",
    "# 1. 코드 분석 실행. 어떤 코드고, 리팩토링하면 좋은 것 까지.\n",
    "review_chain = code_review_messages | llama_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 해당 코드 분석을 포멧에 맞게 변환.\n",
    "translate_chain = translate_messages | mistral_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 체인 실행\n",
    "final_chain = {\"review_result\" : review_chain} | translate_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 리턴\n",
    "result = final_chain.invoke({\n",
    "    \"prompt\" : user_prompt\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일생성\n",
    "file_dir = \"./\"\n",
    "file_name = \"example.md\"\n",
    "file_info = save_md_to_file(result.content, file_dir, file_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
